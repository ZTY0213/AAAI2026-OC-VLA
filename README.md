# Grounding actions in camera space: Observation-centric vision-language-action policy

[![arXiv](https://img.shields.io/badge/arXiv-2508.13103-df2a2a.svg)](http://arxiv.org/abs/2508.13103) 
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

This is an official repo of AAAI 2026 paper "Grounding actions in camera space: Observation-centric vision-language-action policy".

## Code

You can find the code from the repo of [Dita](https://github.com/RoboDita/Dita).

## Acknowledgement

If you have any questions, feel free to contact Tianyi Zhang (tianyizhang0213 at zju dot edu dot cn), Haonan Duan (duan.haonan10 at gmail dot com) or Zhi Hou (houzhi91 at gmail dot com).



## Citation

If you find our code or models useful in your work, please consider to cite our paper:

```
@article{zhang2025grounding,
  title={Grounding actions in camera space: Observation-centric vision-language-action policy},
  author={Zhang, Tianyi and Duan, Haonan and Hao, Haoran and Qiao, Yu and Dai, Jifeng and Hou, Zhi},
  journal={arXiv preprint arXiv:2508.13103},
  year={2025}
}

@article{hou2025dita,
 title={Dita: Scaling Diffusion Transformer for Generalist Vision-Language-Action Policy},
 author={Hou, Zhi and Zhang, Tianyi and Xiong, Yuwen and Duan, Haonan and Pu, Hengjun and Tong, Ronglei and Zhao, Chengyang and Zhu, Xizhou and Qiao, Yu and Dai, Jifeng and Chen, Yuntao},
 journal={arXiv preprint arXiv:2503.19757},
 year={2025}
}
```
